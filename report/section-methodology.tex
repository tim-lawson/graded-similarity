\section{Methodology}
\label{sec:methodology}

\subsection{Embedding models}

I undertook this task to investigate the relative performance of pre-trained static and
contextual embeddings for a context-dependent word-similarity task.
The baseline models for the task were the multilingual BERT model
\parencite{Devlin2019} and ELMo models \parencite{Peters2018a} trained on Finnish,
Croatian, and Slovene datasets \parencite{Ulcar2020a},\footnote{I did not directly
  reproduce the baseline models because the first requires the \texttt{bert-embedding}
  Python package, which has been deprecated since 2020 and is incompatible with Apple's
  ARM-based processors \parencite{Lai2023}.
  However, it is notionally equivalent to the contextual embeddings of the
  \texttt{bert-base-multilingual-cased} model with a window size of zero.
} and the vast majority of the task
submissions were based on Transformers \parencite[36,42-45]{Armendariz2020a}.
Therefore, I chose to evaluate a variety of pre-trained Transformers.
Because both static and contextual embeddings can be obtained from a Transformer model,
this approach facilitated a direct comparison between them.
The models that I evaluated were accessed via the HuggingFace \emph{Transformers}
library \parencite{Wolf2020a} and are listed in \cref{table:language-models}.

\begin{figure}
  \centering
  \begin{tabular}{lcccc}
    Model name                                                        & English    & Finnish    & Croatian   & Slovene
    \\
    \hline
    \texttt{EMBEDDIA/crosloengual-bert}\textsuperscript{1}            & \checkmark & \checkmark &
    \checkmark                                                        & \checkmark
    \\
    \texttt{TurkuNLP/bert-base-finnish-cased-v1}\textsuperscript{2}   &            & \checkmark &            &
    \\
    \texttt{TurkuNLP/bert-base-finnish-uncased-v1}\textsuperscript{2} &            & \checkmark &            &
    \\
    \texttt{TurkuNLP/bert-large-finnish-cased-v1}\textsuperscript{2}  &            & \checkmark &            &
    \\
    \texttt{bert-base-cased}                                          & \checkmark &            &            &
    \\
    \texttt{bert-base-multilingual-cased}                             &
    \checkmark                                                        & \checkmark & \checkmark & \checkmark
    \\
    \texttt{bert-base-multilingual-uncased}                           & \checkmark & \checkmark & \checkmark &
    \checkmark
    \\
    \texttt{bert-base-uncased}                                        & \checkmark &            &            &
    \\
    \texttt{bert-large-cased}                                         & \checkmark &            &            &
    \\
    \texttt{bert-large-cased-whole-word-masking}                      & \checkmark &            &            &
    \\
    \texttt{bert-large-uncased}                                       & \checkmark &            &            &
    \\
    \texttt{bert-large-uncased-whole-word-masking}                    & \checkmark &            &            &
    \\
    \texttt{classla-bcms-bertic}\textsuperscript{3}                   &            &            & \checkmark &
  \end{tabular}
  \caption{The pre-trained models from the HuggingFace \emph{Transformers} library
    \parencite{Wolf2020a} that I evaluated for each language.
    The corresponding references are \textsuperscript{1}\textcite{Ulcar2020},
    \textsuperscript{2}\textcite{Virtanen2019}, \textsuperscript{3}\textcite{Ljubesic2021},
    and \textcite{Devlin2019} otherwise.
  }
  \label{table:language-models}
\end{figure}

The primary comparison that I made was between these models' static input and
contextual output representations.
Several of the task submissions used a combination of a Transformer's hidden-states
\parencites[e.g.][276]{Gamallo2020}[61]{CostellaPessutto2020}[145]{Hettiarachchi2020}.
This method is supported by the analysis of \textcite{Ethayarajh2019}, who found that
the upper layers of Transformer models produce more context-dependent representations.
Hence, I also evaluated an example of this method -- a thorough comparison of its
variants is, however, beyond the scope of this paper.
\begin{samepage}
  Hereafter, I refer to the three kinds of embeddings that I evaluated as:
  \begin{itemize}
    \item \emph{static}, the model's input embeddings;
    \item \emph{contextual}, the model's output embeddings; and
    \item \emph{pooled}, the sum of the model's last four hidden-states.
  \end{itemize}
\end{samepage}

\subsection{Composition operations}

The basic procedure that I employed is described in \cref{chart:schematic-procedure}.
For each pair of target words and each of the two contexts in which they appear, I
obtained a contextualized representation of a target word by:
\begin{enumerate}
  \item finding the index of the target word's first sub-word token within the tokens of its context;
  \item finding the tokens within a fixed-size window around its first token;
  \item obtaining the embeddings of the tokens in the window; and
  \item composing the embeddings to produce a single representation.
\end{enumerate}
Notably, the use of a sub-word vocabulary by the models in question
\parencite[e.g.,][4174]{Devlin2019} dictates that a target word may be represented by a
different number of tokens in each context.
As a result, the similarity between the representations of a pair of target words may
be different in each context, even if the representations are individual static
embeddings.
This is the cause of the non-zero scores obtained by models of this kind, particularly
for the Finnish language (\cref{sec:language-specificity}).

\begin{figure}
  \centering
  \newcommand*{\orawidest}{accept}
  \newcommand*{\oratallest}{\#\#}
  \newlength{\orawidth}
  \settowidth{\orawidth}{\orawidest}
  \newcommand*{\ora}[1]{\overrightarrow{#1\vphantom{\oratallest}}}
  \begin{tikzpicture}[
      every path/.style = {thick, ->},
      every node/.style = {inner sep = 0, outer sep = 0.05in},
      node distance = 0.6in
    ]
    \node [label] (a) {\dots from the Isenj. He \underline{accepts} their offer but knows that \dots };
    \node [label, below of=a] (b) {$\left[ \dots\,,\ \text{\#\#nj}\,,\ .\,,\ \text{he}\,,\ \text{\underline{accept}}\,,\ \text{\#\#s}\,,\ \text{their}\,,\ \text{offer}\,,\ \dots \right]$};
    \node [label, below of=b] (c) {$\left[ \, \text{he}\,,\ \text{accept}\,,\ \text{\#\#s} \, \right]$};
    \node [label, below of=c] (d) {$\left[ \ \ora{\text{he}}\,,\ \ora{\text{accept}}\,,\ \ora{\text{\#\#s}} \ \right]$};
    \node [label, below of=d] (e) {$ \ora{\textit{accept}} = \ora{\text{he}} + \ora{\text{accept}} + \ora{\text{\#\#s}}$};
    \draw (a) -- (b) node[midway, right=0.2in] {1. Find first sub-word token};
    \draw (b) -- (c) node[midway, right=0.2in] {2. Find tokens in a fixed-size window};
    \draw (c) -- (d) node[midway, right=0.2in] {3. Obtain embeddings};
    \draw (d) -- (e) node[midway, right=0.2in] {4. Compose embeddings};
  \end{tikzpicture}
  \caption{A schematic of the procedure used to
    obtain a contextualized representation of a target word from pre-trained embeddings.
    In this example, the target word is ``accept'', the window size is one (either side of
    the target word), and the composition operation is addition.
  }
  \label{chart:schematic-procedure}
\end{figure}

Inspired by \textcite{Landauer1997}, \textcite{Kintsch2001}, and
\textcite{Mitchell2008}, I predominantly investigated element-wise addition\footnote{
  The cosine similarity between two vectors is invariant with respect to the
  multiplication of the vectors by scalars.
  Therefore, the results of composing the embeddings within a fixed-size context window
  by addition or the arithmetic mean are equal.
  Hence, I did not also investigate the arithmetic mean.
} and
multiplication as composition operations to contextualize embeddings.
However, preliminary experiments indicated that multiplication performed poorly across
all languages, models, and window sizes; hence, it was discarded before the final
analysis.
Initially, I also investigated the concatenation (`stacking') of embeddings.
In the case that the number of embeddings was fewer than that expected for the window
size, i.e., the target word was too close to the beginning or end of its context, I
right-padded the concatenated embedding with zeros to obtain contextual embeddings of
equal length.
This approach was also inferior to addition for practically all combinations of
parameters.

\subsection{Window size}

Due to the computational expense of exhaustively searching the possible window sizes, I
applied heuristics to constrain the search space.
A na√Øve estimation of the average number of words in each context, i.e., segmenting on
whitespace, gave a result of between $40$ and $60$ for the different languages.
Therefore, for the static-embedding models, I chose $50$ as an upper bound on the
window size on either side of the target word.
The motivation to choose a smaller maximum window size for contextual-embedding models
was similarly economical (\cref{sec:cost-benefit}); however, as the window size
approaches the length of the sequence, one would expect a combination of token
representations to be superseded by the sequence-level representation of the model,
e.g., the special \texttt{CLS} token of BERT models \parencite[4174]{Devlin2019}.
These heuristics were largely vindicated by the results of the evaluation, which
demonstrated that the scores decrease as the window size approaches the maximum.
