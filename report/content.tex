\pgfplotstableset{
  col sep=comma,
  columns={model_name,context_window_size,context_window_operation,score},
  column type=l,
  every head row/.style={before row=\hline,after row=\hline},
  every last row/.style={after row=\hline},
  skip rows between index={0}{1},
  skip rows between index={6}{1000},
  columns/model_name/.style={
      column name=Model,
      string type,
      postproc cell content/.append style={
          @cell content={\texttt{##1}}
        }
    },
  columns/context_window_size/.style={
      column name=Window size,
      int detect
    },
  columns/context_window_operation/.style={
      column name=Operation,
      string type
    },
  columns/score/.style={
      column name=Score,
      fixed,
      fixed zerofill,
      precision=3,
    },
}

\section{Introduction}

In his \emph{Foundations of Arithmetic}, Frege promises ``never to ask for the meaning
of a word in isolation, but only in the context of a proposition''
\parencite*[xvii]{Frege1980}.
This `context principle' is intuitive: words are frequently polysemous, or assume
different connotations and emphasis within different expressions.
Historically, however, contextuality has been a problem for distributional meaning
representations.
Founded on the distributional hypothesis \parencites{Harris1954}{Firth1957}, both
count-based and predictive models of word meaning\footnote{ This terminological
  distinction is due to \textcite{Baroni2014a}.
} originally
produced a single representation for each word in the model's vocabulary.
One of these \emph{static} representations must, therefore, encode all of a word's
senses and connotations, which precludes its use in modelling context-dependent
phenomena.

Prior to the widespread availability of pre-trained word embeddings
\parencites[e.g.,][]{Mikolov2013}{Pennington2014}, this problem was generally addressed
by one of two approaches: firstly, by producing a representation for each sense of a
target word and disambiguating between them in the given context; or secondly, by
composing the representation of the target word with the representations of the words
in its context.
These approaches have been largely overshadowed by the advent of model architectures
that take sequences as inputs and naturally produce \emph{contextual} representations
of the items in the sequence, such as transformers \parencite{Vaswani2017}.

To my knowledge, however, there has been scant direct comparison of the performance of
these contextual representations with the application of prior methods of
contextualisation to static representations.
SemEval-2020 Task 3, \emph{Graded Word Similarity in Context}
\parencite{Armendariz2020a}, presents an opportunity to make such a comparison.
Briefly, the task is to predict the change in the human judgment of similarity between
the same pair of words in two different contexts.
This objective is both context-dependent and continuous, i.e., not limited to discrete
word-sense disambiguation.
I elected to focus on the first subtask, which is to predict the \emph{change} in
similarity, rather than the similarity in each context.
Specifically, I compared the results of computing the similarity between both kinds of
representation of the target words and their composition with the representations of
the words in a fixed-size context window, inspired by \textcite{Kintsch2001} and
\textcite{Mitchell2008}.

\section{Task definition}

The CoSimLex dataset \parencite{Armendariz2020}, which served to evaluate the task
submissions, extends the SimLex-999 dataset \parencite{Hill2015} to include multiple
contexts for each pair of words.

\section{Related work}

\section{Methodology}

\section{Results}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        title=Time vs. score,
        xlabel={Score},
        ylabel={Time (s)},
      ]
      \addplot+[only marks, mark size=1.0pt] table[
          col sep=comma
        ]{../results/543d797b/subtask1/static_en.csv};
      \addplot+[only marks, mark size=1.0pt] table[
          col sep=comma
        ]{../results/543d797b/subtask1/static_fi.csv};
      \addplot+[only marks, mark size=1.0pt] table[
          col sep=comma
        ]{../results/543d797b/subtask1/static_hr.csv};
      \addplot+[only marks, mark size=1.0pt] table[
          col sep=comma
        ]{../results/543d797b/subtask1/static_sl.csv};
    \end{axis}
  \end{tikzpicture}

  \caption{The elapsed time for each static-embedding model against its score.}
\end{figure}

\begin{figure}
  \centering
  \pgfplotstabletypeset{../results/5ae22ab/subtask1/static_en.csv}
  \caption{The top five results in English with static embeddings.}
\end{figure}

\begin{figure}
  \centering
  \pgfplotstabletypeset{../results/543d797b/subtask1/static_fi.csv}
  \caption{The top five results in Finnish with static embeddings.}
\end{figure}

\begin{figure}
  \centering
  \pgfplotstabletypeset{../results/543d797b/subtask1/static_hr.csv}
  \caption{The top five results in Croatian with static embeddings.}
\end{figure}

\begin{figure}
  \centering
  \pgfplotstabletypeset{../results/543d797b/subtask1/static_sl.csv}
  \caption{The top five results in Slovakian with static embeddings.}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        title=Time vs. score,
        xlabel={Score},
        ylabel={Time (s)},
      ]
      \addplot+[only marks, mark size=1.0pt] table[
          col sep=comma
        ]{../results/543d797b/subtask1/contextual_fi.csv};
      \addplot+[only marks, mark size=1.0pt] table[
          col sep=comma
        ]{../results/543d797b/subtask1/contextual_hr.csv};
      \addplot+[only marks, mark size=1.0pt] table[
          col sep=comma
        ]{../results/543d797b/subtask1/contextual_sl.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{The elapsed time for each contextual-embedding model against its score.}
\end{figure}

\section{Conclusion}

\begin{itemize}
  \item Language-specific models perform better on their target language(s).
  \item A small context window improves the outcomes for both static and contextualised embeddings.
  \item The contextualised embeddings outperform the static embeddings.
  \item However, the static embeddings are much quicker to compute.
\end{itemize}
