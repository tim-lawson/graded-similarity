\pgfplotstableset{
  col sep=comma,
  columns={model,model_name,language,window,operation,similarity,score,time},
  column type=l,
  every head row/.style={before row=\hline,after row=\hline},
  every last row/.style={after row=\hline},
  skip rows between index={0}{1},
  columns/model/.style={
      column name=Model,
      string type
    },
  columns/model_name/.style={
      column name=Model name,
      string type,
      postproc cell content/.append style={
          @cell content={\texttt{##1}}
        }
    },
  columns/language/.style={
      column name=Language,
      string type,
      postproc cell content/.append style={
          @cell content={\texttt{##1}}
        }
    },
  columns/window/.style={
      column name=Window size,
      int detect
    },
  columns/operation/.style={
      column name=Operation,
      string type
    },
  columns/similarity/.style={
      column name=Similarity measure,
      string type
    },
  columns/score/.style={
      column name=Score,
      fixed,
      fixed zerofill,
      precision=3,
    },
  columns/time/.style={
      column name=Time (s),
      fixed,
      fixed zerofill,
      precision=3,
    },
}

\section{Introduction}

In his \emph{Foundations of Arithmetic}, Frege promises ``never to ask for the meaning
of a word in isolation, but only in the context of a proposition''
\parencite*[xvii]{Frege1980}.
This `context principle' is intuitive: words are frequently polysemous, or assume
different connotations and emphasis within different expressions.
Historically, however, contextuality has been a problem for distributional meaning
representations.
Founded on the distributional hypothesis \parencites{Harris1954}{Firth1957}, both
count-based and predictive models of word meaning\footnote{ This terminological
  distinction is due to \textcite{Baroni2014a}.
} originally
produced a single representation for each word in the model's vocabulary.
One of these \emph{static} representations must, therefore, encode all of a word's
senses and connotations, which precludes its use in modelling context-dependent
phenomena.

Prior to the widespread availability of pre-trained word embeddings
\parencites[e.g.,][]{Mikolov2013}{Pennington2014}, this problem was generally addressed
by one of two approaches: firstly, by producing a representation for each sense of a
target word and disambiguating between them in the given context (\emph{word-sense
  disambiguation}); or secondly, by composing the representation of the target word with
the representations of the words in its context (\emph{contextualisation}).
These approaches have been largely overshadowed by the advent of model architectures
that take sequences as inputs and naturally produce \emph{contextual} representations
of the items in the sequence, such as transformers \parencite{Vaswani2017}.

To my knowledge, however, there has been scant direct comparison of the performance of
these contextual representations with the application of prior methods of
contextualisation to static representations.
SemEval-2020 Task 3, \emph{Graded Word Similarity in Context}
\parencite{Armendariz2020a}, presents an opportunity to make such a comparison.
Briefly, the task is to predict the continuously-valued human judgment of similarity of
the same pair of words in two different contexts (Section~\ref{task-definition}).
I elected to focus on the first subtask, which is to predict the \emph{change} in
similarity, rather than the similarity in each context.
Specifically, I compared the results of computing the similarity between both kinds of
representation of the target words and their composition with the representations of
the words in a fixed-size context window, inspired by \textcite{Kintsch2001} and
\textcite{Mitchell2008}.

\section{Task definition}
\label{task-definition}

The CoSimLex dataset \parencite{Armendariz2020}, which served to evaluate the task
submissions, extends the SimLex-999 dataset \parencite{Hill2015} to include multiple
contexts for each pair of words.

\section{Related work}

\section{Methodology}

\section{Results}

\begin{figure}
  \centering
  \pgfplotstabletypeset[
    columns={model,model_name,score},
    sort,
    sort cmp=string <,
    sort key=model_name,
    sort key=model,
    every row 10 column 2/.style={
        postproc cell content/.style={
            @cell content/.add={$}{^\dagger$}
          }
      },
  ]{../results/model=static+contextual_language=fi_window=0_operation=none_similarity=cosine.csv}
  \caption{The results in Finnish with a zero-size context window.}
\end{figure}

\import{.}{figure-score-window}

\section{Conclusion}

\begin{itemize}
  \item Language-specific models perform better on their target language(s).
  \item A small context window improves the outcomes for both static and contextualised embeddings.
  \item The contextualised embeddings outperform the static embeddings.
  \item However, the static embeddings are much quicker to compute.
\end{itemize}
