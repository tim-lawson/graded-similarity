\section{Results}
\label{sec:results}

\subsection{Hyperparameter search}
\label{sec:hyperparameter-search}

\input{figure-score-practice.tex}

In \cref{sec:cost-benefit,sec:language-specificity}, I present the results for
different models on the \emph{evaluation} dataset.
However, it would not have been possible or legitimate as a task submission to optimize
hyperparameters on the evaluation dataset.
Therefore, I also optimized them on the `practice kit' dataset
(\cref{sec:task-definition}) to select a candidate model for each language and kind of
embedding.
This data was not provided for Finnish, so it was excluded from the analysis.
In comparison to addition, the scores for the other composition operations varied more
widely with respect to the window size
(\cref{chart:score-window-static-practice,chart:score-window-contextual-practice,chart:score-window-pooled-practice}).
Hence, I excluded them before selecting candidate models.
The models and their scores on the two datasets are listed in
\cref{table:practice-best-score}.
As expected, the scores on the `practice kit' dataset of fewer instances are generally
higher than those on the evaluation dataset.
In some cases, the scores on the evaluation dataset are close to the maxima in
\cref{table:evaluation-best-score}.
Generally, the benefit of additional `training' data is evident.

\input{table-practice-best-score.tex}

\subsection{Language-specificity of window-size dependence}
\label{sec:language-specificity}

Generally, I found that the scores obtained by all three types of embeddings were
maximized by a non-zero context-window size
(\cref{table:practice-best-score,table:evaluation-best-score}).
The influence of the window size is intuitive in the case of static embeddings.
Without a context window, the representations of a target word only differ between
contexts if the word is represented by different sub-word tokens in the different
contexts.
A similar argument applies to contextual embeddings, in that a target word may be
represented by multiple sub-word tokens that differ between contexts.
For the composition operation of addition, the scores against window size for each
language, kind of embedding, and model are shown in
\cref{chart:score-window-static,chart:score-window-contextual,chart:score-window-pooled}.

\input{figure-score-window.tex}

\textcites[3]{Virtanen2019} have noted that, for a random sample of 1\% of the relevant
Wikipedia dataset, the number of sub-word tokens that represent a word
is greater for Finnish ($1.97$) than for English ($1.16$) with the multilingual BERT model.
This is attributed to the morphological complexity of Finnish and its comparatively
small fraction of the model's vocabulary.
Accordingly, I found that Finnish-specific models generally outperformed multilingual
ones and that the scores varied more widely with window size for Finnish than the other
languages.

\newpage

\subsection{Cost-benefit analysis of contextual embeddings}
\label{sec:cost-benefit}

\input{figure-prediction.tex}

\input{figure-score-time.tex}

In the main, greater scores were achieved with contextual and pooled embeddings than
with static embeddings (\cref{table:practice-best-score,table:evaluation-best-score}).
However, static embeddings make up a small fraction of the size of a contextual
language model.
For example, BERT's vocabulary size is approximately $30000$, the dimensions of the
\texttt{bert-base} and \texttt{bert-large} variants' hidden-states are $768$ and
$1024$, and their total parameters are $110$M and $340$M respectively
\parencites[4173-4174]{Devlin2019}.
Static embeddings thus make up approximately $21$\% and $9$\% of the total parameters.
It is also much faster to compute a contextualized representation from static
embeddings than to run inference on a language model.
For a na√Øve implementation of the procedure described in \cref{sec:methodology}, the
approximate time taken to compute the change in similarity between two words in context
is shown in \cref{chart:score-time}.
It is notably greater for contextual embeddings, and the right-most cluster is due to
the \texttt{large} model variants.

\input{table-evaluation-best-score.tex}

I quantified the significance of the differences between the scores obtained with
different kinds of embeddings by paired $t$-tests and the Nemenyi test
\parencites{Demsar2006} on the scores of the best models over ten random samples of
90\% of the evaluation dataset (\cref{table:evaluation-best-score}).
For each pair of models, the null hypothesis was that the differences between the mean
scores were due to chance.
For each language, either contextual or pooled embeddings significantly outperformed
static embeddings, but did not differ significantly from each other
(\cref{table:significance}).
Hence, the results support the conclusion that contextual embeddings are more effective
than static embeddings for this task.

\input{table-significance.tex}