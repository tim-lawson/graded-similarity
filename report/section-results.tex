\section{Results}
\label{sec:results}

\subsection{Hyperparameter search}
\label{sec:hyperparameter-search}

In \cref{sec:cost-benefit,sec:language-specificity,sec:significance}, I present the
results of different models on the evaluation dataset.
However, by comparing them, I have effectively performed hyperparameter search on the
evaluation dataset, which would not have been possible or legitimate as a task
submission.
Therefore, I also carried out this procedure on the `practice kit' dataset
(\cref{sec:task-definition}) to select a candidate model for each language and kind of
embedding.
A `practice kit' was not provided for Finnish, so it was excluded from the analysis.
As discussed in \cref{sec:composition-operations}, multiplication and concatenation
performed poorly in comparison to addition.
This is shown by the variability of the score with respect to the window size in
\cref{chart:score-window-static-practice,chart:score-window-contextual-practice,chart:score-window-pooled-practice}.
Hence, I excluded the other composition operations to select candidate models.
The candidate models and their scores on the `practice kit' and evaluation datasets are
given in \cref{table:practice-best-score}.

\import{}{figure-score-practice.tex}

\import{}{table-practice-best-score.tex}

\subsection{Cost-benefit analysis of contextual embeddings}
\label{sec:cost-benefit}

In the main, greater scores were achieved with contextual and pooled embeddings than
with static embeddings (\cref{table:practice-best-score,table:evaluation-best-score}).
However, static embeddings make up a small fraction of the size of a contextual
language model.
For example, BERT's vocabulary size is approximately $30000$.
The dimensions of the \texttt{bert-base} and \texttt{bert-large} variants'
hidden-states are $768$ and $1024$, and their total parameters are $110$M and $340$M
\parencite[4173-4174]{Devlin2019}.
Static embeddings thus make up approximately $23$M and $31$M or $21$\% and $9$\% of the
total parameters.
It is also much faster to compute a contextualized representation from static
embeddings than to run inference on a language model.
For a na√Øve implementation of the procedure described in \cref{sec:methodology}, the
approximate time taken to compute the change in similarity between two words in
context, i.e., the total time divided by the number of instances, is notably greater
for contextual embeddings than for static embeddings.
This is shown in \cref{chart:score-time}, where the right-most cluster is due to the
\texttt{large} model variants.
\\
\todonum{Plot the gold-standard values against the predictions for the best model
  for each language to visualize the Pearson correlation coefficient.}

\import{}{figure-score-time.tex}

\subsection{Language-specificity of window-size effects}
\label{sec:language-specificity}

\import{}{table-evaluation-best-score.tex}

Generally, I found that the scores obtained by all three types of embeddings were
maximized by a non-zero context-window size.
The influence of the window size is intuitive in the case of static embeddings.
Without a context window, the representations of a target word only differ between
expressions if the word is represented by different sub-word tokens in the different
expressions.
A similar argument applies to contextual embeddings, in that a target word may be
represented by multiple sub-word tokens.
For the composition operation of addition, the scores against window size for each
language and model are given in
\cref{chart:score-window-static,chart:score-window-contextual,chart:score-window-pooled}.

\textcite[3]{Virtanen2019} noted that, for a random sample of 1\% of the relevant
Wikipedia dataset, the number of sub-word tokens used to represent a word by a
multilingual BERT model is greater for Finnish ($1.97$) than for English ($1.16$).
This is attributed to the morphological complexity of Finnish and its comparatively
small fraction of the multilingual model's vocabulary.
Accordingly, I found that Finnish-specific models generally outperformed multilingual
ones and that the score varied more widely with window size for Finnish than the other
languages.

\import{}{figure-score-window.tex}

\subsection{Significance tests}
\label{sec:significance}

Finally, to quantify the significance of the differences between the scores obtained
with different kinds of embeddings, I computed paired $t$-tests of the scores of the
best models in each class over the same ten random samples of 90\% of the evaluation
dataset.
For each pair of model classes, the null hypothesis is that the two sets of scores have
the same mean, i.e., there is no significant difference between the mean score obtained
by the two classes.
\\
\todonum{Add significance test results.}