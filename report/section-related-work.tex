\section{Related work}
\label{sec:related-work}

\subsection{Composition and contextualization}

Many context-based approaches to composition and contextualization have been proposed
since the advent of count-based models of word meaning.
With reference to Latent Semantic Analysis \parencite{Deerwester1990}, for example,
\citeauthor{Landauer1997} argued that taking the average of the high-dimensional
representation of a word and the representations of the words in its context may
suffice to determine the word's contextual meaning \parencite*[229-230]{Landauer1997}.
Thus, the contextualization of representations of word meanings is intimately related
to their composition to form representations of more complex expressions.
This relationship is evident, for example, in the work of \textcite{Kintsch2001}, who
proposed a procedure to contextualize the representation of a predicate according to
its argument, and in the adaptation of this demonstration by \textcite{Mitchell2008} to
evaluate alternative composition operations.
Vector addition and averaging continue to be `surprisingly effective' means to compose
word embeddings \parencite[10]{Boleda2020}, and addition produces plausible results for
the word-analogy task \parencites[9]{Mikolov2013}[7]{Mikolov2013a}.
Recent reviews of distributional semantic models are given by
\textcites{Lenci2018}{Boleda2020}.

\subsection{Costs and benefits of contextual models}

Contextual language models have achieved widespread success on benchmark tasks
\parencite[22-27]{Bommasani2022}.
There is, however, cause to criticize the suitability of typical benchmarks for
characterizing the capabilities of language models \parencite[5-6]{Srivastava2023}.
Furthermore, the social and environmental costs of deploying a large model may not be
justifiable, and the necessary computational resources may be prohibitive to an
organization or in a resource-constrained environment
\parencite[142-145,154]{Bommasani2022}.
In the case of systems based on contextual embeddings, for instance,
\textcite{Arora2020} have shown that static and even \emph{random} embeddings can
achieve similar performance, given sufficient data and linguistically simple tasks.
Relatedly, \textcites[5244-5246]{Gupta2019}[4760-4762]{Bommasani2020} have compared
different kinds of embeddings for word-similarity tasks and found that static
embeddings can be obtained from contextual models that outperform their contextual
counterparts while reducing the cost of inference.
Surveys of contextual and static embeddings are given by
\textcites{Liu2020}{Torregrossa2021}, and further analyses of contextual language
models by \textcites{Reif2019}{Brunner2019}, for example.
The costs and benefits of contextual models in this context are discussed in
\cref{sec:cost-benefit}.

\subsection{Word similarity}

\textcite{Batchkarov2016} critically analyse word similarity as an evaluation
methodology for distributional semantic models.
In particular, the notion of `similarity' manifested by these models is ambiguous
\parencite{Elekes2020} and encompasses a broad range of semantic relations
\parencite[2]{Pado2003}, with the consequence that performance on an intrinsic
word-similarity task does not necessarily translate to extrinsic downstream tasks
\parencite[7-8]{Batchkarov2016}.
Moreover, inter-annotator agreement is generally poor for word-similarity in comparison
to more specific tasks \parencite[8-9]{Batchkarov2016}.
In this case, \textcites[8]{Armendariz2020}[42]{Armendariz2020a} reported similar
inter-annotator correlations between the different languages and to those of the
SimLex-999 dataset \parencite[678-680]{Hill2015}.
In the present context, we are explicitly concerned with the ability of pre-trained
embeddings to capture context-dependent similarity judgments.
However, the interpretation of distributional semantic models as explanatory theories
of human linguistic processing is subject to debate \parencite{Gunther2019}, and it may
be that less data-intensive models are more appropriate for a specific task of this
kind \parencite{DeDeyne2016}.
