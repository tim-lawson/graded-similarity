\section{Related work}
\label{sec:related-work}

Many context-based approaches to word-sense disambiguation have been proposed since the
advent of count-based models of word meaning.
In \textcite{Landauer1997}'s introduction of Latent Semantic Analysis (LSA), the authors
argued that taking the average of the high-dimensional representation of a word with
those of its immediate context may be sufficient to determine the word's contextual
meaning \parencite*[229-230]{Landauer1997}.
The contextualisation of representations of word meanings is intimately related to their
composition to form representations of phrase and sentence meanings.
This connection is evident in \textcite{Kintsch2001}'s proposal of a procedure to
modify the vector of a predicate according to its argument in a given context.
\textcite{Mitchell2008} also adapted \citeauthor{Kintsch2001}'s demonstration to
evaluate alternative composition operations.

Vector addition and averaging continue to be `surprisingly effective' means to
compose word embeddings \parencite[10]{Boleda2020}, and addition produces plausible
results for the word-analogy task\footnote{
  Also known as the parallelogram model of analogy \parencite{Rumelhart1973}.
} \parencites[e.g.,][9]{Mikolov2013}[7]{Mikolov2013a}.

Nevertheless, models that produce contextual embeddings have achieved widespread
success on benchmarks that involve language understanding.
As \textcite{Arora2020} point out, this comes at a significant computational cost, and
static embeddings may be sufficient for many tasks.
Furthermore, \textcite{Gupta2019} and \textcite{Bommasani2020} have shown that
static embeddings can be obtained from contextual-embedding models, which outperform
the embeddings of static models while retaining their computational advantages.

\todonum{Finish this section.}