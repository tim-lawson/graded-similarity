\section{Related work}
\label{sec:related-work}

\subsection{Composition and contextualization}

Many approaches to composition and contextualization have been proposed since the
advent of count-based models of word meaning.
For example, with reference to Latent Semantic Analysis \parencites{Deerwester1990},
\citeauthor{Landauer1997} argued that taking the average of the high-dimensional
representation of a word and the representations of the words in its context may
suffice to determine the word's contextual meaning \parencites*[229-230]{Landauer1997}.
The relationship between composition and contextualization is evident in the work of
\textcites{Kintsch2001}, who proposed a procedure to contextualize the representation
of a predicate according to its argument, and its adaptation by
\textcites{Mitchell2008} to evaluate alternative composition operations.

Vector addition and averaging continue to be `surprisingly effective' means to compose
word embeddings \parencites[10]{Boleda2020}, and addition produces plausible results
for the word-analogy task \parencites[9]{Mikolov2013}[7]{Mikolov2013a}, though its
generality as an evaluation methodology has been questioned
\parencites[1300]{Lenci2022}.
The relations between distributional semantics and compositionality have been surveyed
by \textcites{Erk2012}{Clark2015}{Boleda2016}.

\subsection{Costs and benefits of contextual models}

Contextual language models have achieved widespread success on benchmark tasks
\parencites[22-27]{Bommasani2022}.
There is, however, cause to criticize the suitability of typical benchmarks for
characterizing the capabilities of language models \parencites[5-6]{Srivastava2023}.
Furthermore, the social and environmental costs of deploying a large model may not be
justifiable, and the necessary computational resources may be prohibitive to a smaller
organization or in a resource-constrained environment
\parencites[142-145,154]{Bommasani2022}.

For instance, \textcites{Lenci2022} found that static embeddings generally outperform
BERT \parencites{Devlin2019} on word-similarity and -association tasks, provided
optimal hyperparameters.
Relatedly, \textcites{Arora2020} have shown that static and even \emph{random}
embeddings can perform comparably to contextual embeddings, which
\textcites[5244-5246]{Gupta2019}[4760-4762]{Bommasani2020} have demonstrated for
word-similarity tasks.
Surveys of contextual and static embeddings are given by
\textcites{Liu2020}{Torregrossa2021}, cross-lingual embeddings by
\textcites{Ruder2019}, and further analyses of contextual language models by
\textcites{Reif2019}{Brunner2019}, for example.
The costs and benefits of contextual models for the task at hand are discussed in
\cref{sec:cost-benefit}.

\subsection{Word similarity}

\textcites{Batchkarov2016} critically analyse word similarity as an evaluation
methodology for distributional semantic models.
In particular, the notion of `similarity' manifested by these models is ambiguous
\parencites{Elekes2020} and encompasses a broad range of semantic relations
\parencites[2]{Pado2003}, with the consequence that performance on an intrinsic
word-similarity task does not necessarily translate to extrinsic downstream tasks
\parencites[7-8]{Batchkarov2016}.
Moreover, inter-annotator agreement is generally poor for word-similarity in comparison
to more specific tasks \parencites[8-9]{Batchkarov2016}.
In this case, \textcites[8]{Armendariz2020}[42]{Armendariz2020a} reported similar
inter-annotator correlations between the different languages and to those of the
SimLex-999 dataset \parencites[678-680]{Hill2015}.

In the present context, we are explicitly concerned with the ability of pre-trained
embeddings to capture context-dependent similarity judgments.
However, the interpretation of distributional semantic models as explanatory theories
of human linguistic processing is subject to debate
\parencites{Gunther2019}{Westera2019}, and it may be that less data-intensive models
are more appropriate for a specific task of this kind \parencites{DeDeyne2016}.
