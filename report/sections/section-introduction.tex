\section{Introduction}
\label{sec:introduction}

In his \emph{Foundations of Arithmetic}, Frege promises ``never to ask for the meaning
of a word in isolation, but only in the context of a proposition''
\parencites*[xvii]{Frege1960}.
This `context principle' is intuitive: words are frequently polysemous, or assume
different connotations and emphasis within different expressions
\parencites[2-3]{Armendariz2020}.
Historically, however, context-dependence has posed a challenge to distributional
semantic models.
Founded on the distributional hypothesis \parencites[e.g.][142-143]{Turney2010}, both
count-based and predictive models\footnote{This terminological distinction is due to
  \textcites{Baroni2014a}.
} originally
produced a single representation for each token in the model's vocabulary.
A \emph{static} embedding of this nature must, therefore, encode all of a word's senses
and connotations, which cannot trivially model context-dependence.

Prior to the widespread availability of pre-trained language models, this problem was
generally addressed by one of two approaches: firstly, by producing a representation
for each sense of a target word and discriminating between them in the given context
(word-sense discrimination); or secondly, by composing the representation of the target
word with the representations of the words in its context (contextualization).
These approaches have been largely overshadowed by the advent of model architectures
that take sequences as inputs and naturally produce \emph{contextual} representations
of the items in the sequence, such as Transformers \parencites{Vaswani2017}.
To my knowledge, however, there has been scant direct comparison of the performance of
these contextual embeddings with the application of prior methods of contextualization
to static embeddings \parencites[see][]{Milajevs2014}.

SemEval-2020 Task 3, ``Graded Word Similarity in Context''
\parencites{Armendariz2020a}, presents an opportunity to make such a comparison.
Briefly, the task is to predict the human judgment of similarity of a pair of words in
two different contexts.
I elected to focus on the first sub-task, which is to predict the \emph{change} in
similarity, rather than the absolute similarity in each context.
Specifically, I evaluated the results obtained by computing the cosine similarity
between the different kinds of embeddings for a variety of pre-trained language models,
and their composition with the embeddings within a fixed-size context
window.\footnote{The code that produced these results is available at
  \github{https://github.com/tslwn/graded-similarity}{tslwn/graded-similarity}.
}
