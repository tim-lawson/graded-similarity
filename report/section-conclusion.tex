\section{Conclusion}
\label{sec:conclusion}

In this paper, I have presented the results of a hypothetical submission to
SemEval-2020 Task 3, ``Graded Word Similarity in Context''.
The purpose of this investigation was to compare the performance of static and
contextual embeddings, and their composition within a fixed-size context window, on the
task of predicting the change in the human judgment of similarity of a pair of words in
two different contexts.
I found that contextual embeddings significantly outperformed static embeddings but at
a computational cost (\cref{sec:cost-benefit}).
Composition benefited both static and contextual embeddings of sub-word tokens, with a
language-dependent optimal window size (\cref{sec:language-specificity}).

These results must be interpreted in context: the original submission authors did not
have access to the evaluation dataset prior to submitting their results, only the
`practice kit' of very few instances, so had limited opportunity to optimize
hyperparameters like the window size (\cref{sec:hyperparameter-search}).
The models that I used were also not necessarily available at the time
(\cref{sec:embedding-models}).
With these caveats, I achieved several notable results (\cref{table:evaluation-best-score}):
\begin{itemize}
  \item The pooled embeddings of \texttt{EMBEDDIA/crosloengual-bert} with a window
        size of three would have placed second among the Croatian submissions, with a
        score of $0.717$.
  \item The contextual embeddings of \texttt{TurkuNLP/bert-base-finnish-uncased-v1}
        with a window size of one would have placed fourth among the Finnish
        submissions, with a score of $0.679$.
\end{itemize}

Word similarity is primarily an intrinsic evaluation task \parencite[1281]{Lenci2022},
as opposed to an extrinsic task directly applicable in a natural language processing
system.
However, the computation of context-dependent similarity is an important aspect of
information retrieval, for instance.
The methods described in this paper could be used to improve the performance of
information retrieval systems that are based on static or contextual embeddings by,
e.g., determining contextually-appropriate synonyms of the keywords in a search query.
The word-similarity task itself could also form part of an assessment of the costs and
benefits of using static or contextual embeddings in a particular application, based on
its resource constraints.
